{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "from statistics import mean, stdev\n",
    "from copy import deepcopy\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "CHECKPOINT_PATH = \"../saved_models\"\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot classification\n",
    "\n",
    "We start our implementation by discussing the dataset setup. In this notebook, we will use 16 different NLP tasks. Each task may have different input (e.g. one or two sentences) and different number of output classes (e.g. 'entailment', 'neutral', 'contradiction' for MNLI, and 'positive'/'negative' for SST2). Instead of splitting the training, validation, and test set over examples, we will split them over datasets: we will use 5 classes for training, and 5 for validation, and 6 for testing. Our overall goal is to obtain a model that can classify the samples in the test datasets with seeing very few examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "First, let's define functions to load the datasets. Next, we need to prepare the dataset in the training, validation and test split as mentioned before. Huggingface gives us the training, validation and test set as separate dataset objects. The next code cells will merge the separate training and validation datasets, and then create the new train-val-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import load_mnli, load_qqp, load_sst2, load_boolq, load_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset super_glue (/Users/FrankVerhoef/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d972c1ca2e94bada3cd9222bb2af2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/FrankVerhoef/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-f6a9e244a16fe613.arrow\n",
      "Loading cached processed dataset at /Users/FrankVerhoef/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-9038dd7cd8f9c87e.arrow\n",
      "Loading cached processed dataset at /Users/FrankVerhoef/.cache/huggingface/datasets/super_glue/cb/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-41619ee74b27d2c7.arrow\n"
     ]
    }
   ],
   "source": [
    "# Define the datasets that we are using for metalearning\n",
    "\n",
    "\n",
    "# TODO: Add other dataloaders\n",
    "# In the meantime just use one dataset for testing purposes\n",
    "\n",
    "DATALOADERS = {\n",
    "#    \"mnli\": load_mnli(),\n",
    "#    \"qqp\": load_qqp(),\n",
    "#    \"sst\": load_sst2(),\n",
    "#    \"wgrande\": \n",
    "#    \"boolq\": load_boolq(),\n",
    "#    \"imdb\": \n",
    "#    \"hswag\":\n",
    "#    \"mrpc\":\n",
    "#    \"argument\": \n",
    "#    \"scitail\": \n",
    "#    \"sociqa\": \n",
    "#    \"cosqa\": \n",
    "#    \"csqa\": \n",
    "#    \"sick\": \n",
    "#    \"rte\": \n",
    "    \"cb\": load_cb()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_train_valid(name):\n",
    "    \n",
    "    (ds, id2label), key = DATALOADERS[name]\n",
    "\n",
    "    # combine input data from train and validation set\n",
    "    all_inputs = torch.cat((ds[\"train\"][\"input_ids\"], ds[key][\"input_ids\"]), dim=0)\n",
    "    all_token_types = torch.cat((ds[\"train\"][\"token_type_ids\"], ds[key][\"token_type_ids\"]), dim=0)\n",
    "    all_masks = torch.cat((ds[\"train\"][\"attention_mask\"], ds[key][\"attention_mask\"]), dim=0)\n",
    "    all_labels = torch.cat((ds[\"train\"][\"labels\"], ds[key][\"labels\"]), dim=0)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": all_inputs, \n",
    "        \"token_type_ids\": all_token_types, \n",
    "        \"attention_mask\": all_masks,\n",
    "        \"labels\": all_labels\n",
    "    }\n",
    "\n",
    "DATASETS = {ds: combine_train_valid(ds) for ds in DATALOADERS.keys()}\n",
    "TASK_IDS = {name: id for id, name in enumerate(DATASETS.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = {\n",
    "    \"tasks\": torch.hstack([\n",
    "        torch.tensor([TASK_IDS[ds_name]] * len(ds[\"labels\"]))\n",
    "        for ds_name, ds in DATASETS.items()\n",
    "    ]),\n",
    "    \"input_ids\": torch.vstack([ds[\"input_ids\"] for ds in DATASETS.values()]),\n",
    "    \"token_type_ids\": torch.vstack([ds[\"token_type_ids\"] for ds in DATASETS.values()]),\n",
    "    \"attention_mask\": torch.vstack([ds[\"attention_mask\"] for ds in DATASETS.values()]),\n",
    "    \"labels\": torch.hstack([ds[\"labels\"] for ds in DATASETS.values()])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, tasks, input_ids, token_type_ids, attention_mask, labels):\n",
    "\n",
    "        super().__init__()\n",
    "        self.tasks = tasks\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task = self.tasks[idx]\n",
    "        x = (self.input_ids[idx], self.token_type_ids[idx], self.attention_mask[idx])\n",
    "        label = self.labels[idx]\n",
    "        return task, x, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_from_tasks(dataset, tasks, **kwargs):\n",
    "    \n",
    "    task_mask = (dataset[\"tasks\"][:,None] == tasks[None,:]).any(dim=-1)\n",
    "    dataset = NLPDataset(\n",
    "        tasks = dataset[\"tasks\"][task_mask], \n",
    "        input_ids = dataset[\"input_ids\"][task_mask], \n",
    "        token_type_ids = dataset[\"token_type_ids\"][task_mask], \n",
    "        attention_mask = dataset[\"attention_mask\"][task_mask], \n",
    "        labels = dataset[\"labels\"][task_mask], \n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For testing purposes just use one small dataset for everything\n",
    "# TODO: Check memory usage! Loading everything caused crash on Colab :(\n",
    "\n",
    "#train_datasets = [\"mnli\", \"qqp\"]\n",
    "#val_datasets = [\"sst\", \"boolq\"]\n",
    "train_datasets = [\"cb\"]\n",
    "val_datasets = [\"cb\"]\n",
    "test_datasets = [\"cb\"]\n",
    "\n",
    "#train_set = dataset_from_tasks(combined_dataset, torch.tensor([TASK_IDS[ds] for ds in train_datasets]))\n",
    "#val_set = dataset_from_tasks(combined_dataset, torch.tensor([TASK_IDS[ds] for ds in val_datasets]))\n",
    "train_set = dataset_from_tasks(combined_dataset, torch.tensor([TASK_IDS[ds] for ds in test_datasets]))\n",
    "val_set = dataset_from_tasks(combined_dataset, torch.tensor([TASK_IDS[ds] for ds in test_datasets]))\n",
    "test_set = dataset_from_tasks(combined_dataset, torch.tensor([TASK_IDS[ds] for ds in test_datasets]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0),\n",
       " (tensor([  101,  2009,  2001,  1037,  3375,  2653,  1012,  2025,  2517,  2091,\n",
       "           2021,  4375,  2091,  1012,  2028,  2453,  2360,  2009,  2001, 20956,\n",
       "           2091,  1012,   102,  1996,  2653,  2001, 20956,  2091,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       "  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "          1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling\n",
    "\n",
    "The strategy of how to use the available training data for learning few-shot adaptation is crucial in meta-learning. At each training step, we randomly select a small number of classes and sample a small number of examples for each class. For the NLP tasks that we have selected, the number of classes is either 2 or 3.\n",
    "\n",
    "This represents our few-shot training batch, which we also refer to as **support set**. Additionally, we sample a second set of examples from the same classes and refer to this batch as **query set**. Our training objective is to classify the query set correctly from seeing the support set and its corresponding labels. The main difference between our three methods (ProtoNet, MAML, and Proto-MAML) is in how they use the support set to adapt to the training classes.\n",
    "\n",
    "This subsection summarizes the code that is needed to create such training batches. In PyTorch, we can specify the data sampling procedure by so-called `Sampler` ([documentation](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)). Samplers are iterable objects that return indices in the order in which the data elements should be sampled. In our previous notebooks, we usually used the option `shuffle=True` in the `data.DataLoader` objects which creates a sampler returning the data indices in random order. Here, we focus on samplers that return batches of indices that correspond to support and query set batches. Below, we implement such a sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotBatchSampler(object):\n",
    "\n",
    "    def __init__(self, dataset_tasks, dataset_targets, N_way, K_shot, include_query=False, shuffle=True, shuffle_once=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            dataset_tasks - PyTorch tensor of the id's of the tasks in the dataset.\n",
    "            dataset_classes - PyTorch tensor of the classes in the dataset\n",
    "            N_way - Number of classes to sample per batch.\n",
    "            K_shot - Number of examples to sample per class in the batch.\n",
    "            include_query - If True, returns batch of size N_way*K_shot*2, which \n",
    "                            can be split into support and query set. Simplifies\n",
    "                            the implementation of sampling the same classes but \n",
    "                            distinct examples for support and query set.\n",
    "            shuffle - If True, examples and classes are newly shuffled in each\n",
    "                      iteration (for training)\n",
    "            shuffle_once - If True, examples and classes are shuffled once in \n",
    "                           the beginning, but kept constant across iterations \n",
    "                           (for validation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset_tasks = dataset_tasks\n",
    "        self.dataset_targets = dataset_targets\n",
    "        self.dataset_task_targets = torch.cat((dataset_tasks.unsqueeze(dim=1), dataset_targets.unsqueeze(dim=1)), dim=1)\n",
    "        self.N_way = N_way\n",
    "        self.K_shot = K_shot\n",
    "        self.shuffle = shuffle\n",
    "        self.include_query = include_query\n",
    "        if self.include_query:\n",
    "            self.K_shot *= 2\n",
    "        self.batch_size = self.N_way * self.K_shot  # Number of overall samples per batch\n",
    "\n",
    "        # Organize examples by task and class\n",
    "        self.tasks = torch.unique(self.dataset_tasks).tolist()\n",
    "        self.classes = {}\n",
    "        self.num_classes = {}\n",
    "        self.indices_per_task = {}\n",
    "        self.batches_per_task = {}\n",
    "        self.indices_per_class = {}\n",
    "        self.batches_per_class = {}\n",
    "        for t in self.tasks:\n",
    "            self.indices_per_task[t] = torch.where(self.dataset_tasks == t)[0].tolist()\n",
    "#            print(\"Indices for task {}: {}\".format(t, self.indices_per_task[t]))\n",
    "#            print(\"Classes for task {}: {}\".format(t, torch.unique(self.dataset_targets[self.indices_per_task[t]]).tolist()))\n",
    "            self.classes[t] = torch.unique(self.dataset_targets[torch.where(self.dataset_tasks == t)[0]]).tolist()\n",
    "            self.num_classes[t] = len(self.classes[t])\n",
    "            self.indices_per_class[t] = {}\n",
    "            self.batches_per_class[t] = {}  # Number of K-shot batches that each class can provide\n",
    "            for c in self.classes[t]:\n",
    "                self.indices_per_class[t][c] = torch.where(\n",
    "                    (self.dataset_tasks == t) *\n",
    "                    (self.dataset_targets == c)\n",
    "                )[0]\n",
    "                print(\"Indices per class ({}, {}): {}\".format(t, c, self.indices_per_class[t][c]))\n",
    "                self.batches_per_class[t][c] = self.indices_per_class[t][c].shape[0] // self.K_shot\n",
    "#                print(\"Batches per class ({}, {}): {}\".format(t, c, self.batches_per_class[t][c]))\n",
    "            self.batches_per_task[t] = sum(self.batches_per_class[t].values())\n",
    "#            print(\"Batches for task {}: {}\".format(t, self.batches_per_task[t]))\n",
    "        self.unique_task_classes = [(t,c) for t in self.tasks for c in self.classes[t]]\n",
    "\n",
    "        # Create a list of task-class tuples from which we select the N classes per batch\n",
    "        self.iterations_per_task = [sum(self.batches_per_class[t].values()) // self.N_way for t in self.tasks]\n",
    "        self.task_list = [t for t in self.tasks for _ in range(self.iterations_per_task[t])]\n",
    "        print(\"Task_list  (init): \", self.task_list)\n",
    "        self.iterations = sum(self.iterations_per_task)\n",
    "#        print(\"Iterations: \", self.iterations_per_task)\n",
    "        self.class_list = {\n",
    "            t: [c for c in self.classes[t] for _ in range(self.batches_per_class[t][c])]\n",
    "            for t in self.task_list\n",
    "        }\n",
    "        print(\"Class_list (init): \", self.class_list)\n",
    "        if shuffle_once or self.shuffle:\n",
    "            self.shuffle_data()\n",
    "        else:\n",
    "            # For testing, we iterate over tasks and classes instead of shuffling them\n",
    "            for t in self.tasks:\n",
    "                sort_idxs = [\n",
    "                    i + p * self.num_classes[t]\n",
    "                    for i, c in enumerate(self.classes[t]) \n",
    "                    for p in range(self.batches_per_class[t][c])\n",
    "                ]\n",
    "                self.class_list[t] = np.array(self.class_list[t])[np.argsort(sort_idxs)].tolist()\n",
    "        print(\"Class_list (final): \", self.class_list)\n",
    "        print(\"Task_list  (final): \", self.task_list)\n",
    "            \n",
    "    def shuffle_data(self):\n",
    "        # Shuffle the examples per task and class.       \n",
    "        for t,c in self.unique_task_classes:\n",
    "            perm = torch.randperm(self.indices_per_class[t][c].shape[0])\n",
    "            self.indices_per_class[t][c] = self.indices_per_class[t][c][perm]\n",
    "\n",
    "        # Shuffle the order of the tasks\n",
    "        random.shuffle(self.task_list)\n",
    "        \n",
    "        # Lastly, shuffle the class list per task.\n",
    "        # Note that this way of shuffling does not prevent to choose the same class twice in a batch. \n",
    "        # Especially with NLP-tasks with small number of classes, this happens quite often  \n",
    "        for t in self.tasks:\n",
    "            random.shuffle(self.class_list[t])\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle data\n",
    "        if self.shuffle:\n",
    "            self.shuffle_data()\n",
    "\n",
    "        # Sample few-shot batches\n",
    "        start_index = defaultdict(int)\n",
    "        task_iter = [0] * len(self.tasks)\n",
    "        for it in range(self.iterations):\n",
    "            \n",
    "            # Select N classes for task t for the batch\n",
    "            t = self.task_list[it]\n",
    "            idx = task_iter[t] * self.N_way\n",
    "            task_iter[t] += 1\n",
    "            class_batch = self.class_list[t][idx:idx+self.N_way]\n",
    "\n",
    "            # For each task-class tuple, select the next K examples and add them to the batch\n",
    "            index_batch = []\n",
    "            for c in class_batch:  \n",
    "                index_batch.extend(self.indices_per_class[t][c][start_index[t,c]:start_index[t,c]+self.K_shot])\n",
    "                start_index[t,c] += self.K_shot\n",
    "                \n",
    "            # If we return support+query set, sort them so that they are easy to split\n",
    "            if self.include_query:\n",
    "                index_batch = index_batch[::2] + index_batch[1::2]\n",
    "            yield [i.item() for i in index_batch]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskBatchSampler(object):\n",
    "\n",
    "    def __init__(self, dataset_tasks, dataset_targets, batch_size, N_way, K_shot, include_query=False, shuffle=True):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            dataset_tasks - PyTorch tensor of the id's of the tasks in the dataset.\n",
    "            dataset_classes - PyTorch tensor of the classes in the dataset\n",
    "            batch_size - Number of tasks to aggregate in a batch\n",
    "            N_way - Number of classes to sample per batch.\n",
    "            K_shot - Number of examples to sample per class in the batch.\n",
    "            include_query - If True, returns batch of size N_way*K_shot*2, which\n",
    "                            can be split into support and query set. Simplifies\n",
    "                            the implementation of sampling the same classes but\n",
    "                            distinct examples for support and query set.\n",
    "            shuffle - If True, examples and classes are newly shuffled in each\n",
    "                      iteration (for training)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.batch_sampler = FewShotBatchSampler(dataset_tasks, dataset_targets, N_way, K_shot, include_query, shuffle)\n",
    "        self.task_batch_size = batch_size\n",
    "        self.local_batch_size = self.batch_sampler.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Aggregate multiple batches before returning the indices\n",
    "        batch_list = []\n",
    "        for batch_idx, batch in enumerate(self.batch_sampler):\n",
    "            batch_list.extend(batch)\n",
    "            if (batch_idx+1) % self.task_batch_size == 0:\n",
    "                yield batch_list\n",
    "                batch_list = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler)//self.task_batch_size\n",
    "\n",
    "    \n",
    "    def get_collate_fn(self):\n",
    "        # Returns a collate function that converts a list of items into format for transformer model\n",
    "        \n",
    "        def collate_fn(item_list):\n",
    "            input_batch = {\n",
    "                \"input_ids\": torch.stack([x[0] for task, x, label in item_list], dim=0),\n",
    "                \"token_type_ids\": torch.stack([x[1] for task, x, label in item_list], dim=0),\n",
    "                \"attention_mask\": torch.stack([x[2] for task, x, label in item_list], dim=0)\n",
    "            } \n",
    "            label_batch = torch.stack([label for task, x, label in item_list], dim=0)\n",
    "            return input_batch, label_batch\n",
    "        \n",
    "        return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock dataset for testing purposes\n",
    "n_c = [2,3,3]\n",
    "c_len = [51,13,10]\n",
    "\n",
    "base = torch.vstack([\n",
    "    torch.tensor([t, c])\n",
    "    for t in range(3)\n",
    "    for c in range(n_c[t])\n",
    "    for e in range(c_len[t])\n",
    "])\n",
    "\n",
    "base = torch.hstack((base, torch.tensor(np.arange(len(base))).unsqueeze(dim=1)))\n",
    "tasks = base[:,0]\n",
    "labels = base[:,1]\n",
    "num = len(base)\n",
    "\n",
    "dataset = NLPDataset(tasks, torch.ones((num, 2)), torch.zeros((num, 2)), torch.zeros((num, 2)), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices per class (0, 0): tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])\n",
      "Indices per class (0, 1): tensor([ 51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
      "         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
      "         93,  94,  95,  96,  97,  98,  99, 100, 101])\n",
      "Indices per class (1, 0): tensor([102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114])\n",
      "Indices per class (1, 1): tensor([115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127])\n",
      "Indices per class (1, 2): tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140])\n",
      "Indices per class (2, 0): tensor([141, 142, 143, 144, 145, 146, 147, 148, 149, 150])\n",
      "Indices per class (2, 1): tensor([151, 152, 153, 154, 155, 156, 157, 158, 159, 160])\n",
      "Indices per class (2, 2): tensor([161, 162, 163, 164, 165, 166, 167, 168, 169, 170])\n",
      "Task_list  (init):  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n",
      "Class_list (init):  {0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 1: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], 2: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]}\n",
      "Class_list (final):  {0: [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0], 1: [2, 1, 0, 1, 0, 0, 0, 2, 0, 1, 2, 0, 1, 2, 1, 1, 2, 2], 2: [0, 2, 0, 1, 0, 1, 2, 1, 1, 0, 0, 2, 2, 1, 2]}\n",
      "Task_list  (final):  [1, 0, 0, 2, 1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "fsb_s = FewShotBatchSampler(tasks, labels, 3, 2, include_query=False, shuffle=False, shuffle_once=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset:    171\n",
      "Classes:            {0: [0, 1], 1: [0, 1, 2], 2: [0, 1, 2]}\n",
      "Number of classes:  {0: 2, 1: 3, 2: 3}\n",
      "Batches per class:  {0: {0: 25, 1: 25}, 1: {0: 6, 1: 6, 2: 6}, 2: {0: 5, 1: 5, 2: 5}}\n",
      "Indices per class:  {0: {0: tensor([41,  3, 42, 19, 35, 49, 40, 29, 15, 16, 11, 26, 22, 21,  6,  0, 45, 44,\n",
      "        25,  7, 17, 18,  5, 48, 38, 32, 28, 12, 20, 23, 31, 43, 30,  1,  2,  4,\n",
      "        36, 47, 13, 37, 33, 39, 46, 50,  9, 10, 14, 34, 24, 27,  8]), 1: tensor([ 97,  99,  92,  65,  78, 101,  76,  81,  66,  94,  56,  73,  55,  70,\n",
      "         64,  69,  58,  75,  62,  61,  63,  79,  77,  60,  53,  67,  71,  52,\n",
      "         82,  84,  95,  72,  74,  96,  51,  57,  89,  85,  86,  83,  90,  80,\n",
      "         87,  54,  98,  68,  88,  59, 100,  93,  91])}, 1: {0: tensor([102, 112, 106, 111, 108, 113, 109, 104, 105, 110, 114, 107, 103]), 1: tensor([123, 125, 121, 119, 122, 118, 120, 126, 116, 115, 127, 117, 124]), 2: tensor([132, 128, 130, 134, 131, 139, 133, 135, 137, 140, 129, 138, 136])}, 2: {0: tensor([145, 149, 147, 146, 142, 150, 143, 144, 141, 148]), 1: tensor([155, 153, 158, 157, 156, 152, 151, 154, 159, 160]), 2: tensor([168, 170, 161, 164, 167, 166, 169, 162, 165, 163])}}\n",
      "Iterations:         27\n",
      "Class list:         {0: [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0], 1: [2, 1, 0, 1, 0, 0, 0, 2, 0, 1, 2, 0, 1, 2, 1, 1, 2, 2], 2: [0, 2, 0, 1, 0, 1, 2, 1, 1, 0, 0, 2, 2, 1, 2]}\n",
      "Print some batches\n",
      "0 [132, 128, 123, 125, 102, 112]\n",
      "1 [41, 3, 97, 99, 92, 65]\n",
      "2 [78, 101, 42, 19, 76, 81]\n",
      "3 [145, 149, 168, 170, 147, 146]\n",
      "4 [121, 119, 106, 111, 108, 113]\n",
      "5 [35, 49, 40, 29, 15, 16]\n",
      "6 [155, 153, 142, 150, 158, 157]\n",
      "7 [66, 94, 11, 26, 22, 21]\n",
      "8 [56, 73, 55, 70, 64, 69]\n",
      "9 [58, 75, 62, 61, 63, 79]\n",
      "10 [161, 164, 156, 152, 151, 154]\n",
      "11 [77, 60, 6, 0, 45, 44]\n",
      "12 [109, 104, 130, 134, 105, 110]\n",
      "13 [25, 7, 53, 67, 17, 18]\n",
      "14 [71, 52, 82, 84, 95, 72]\n",
      "15 [5, 48, 74, 96, 38, 32]\n",
      "16 [28, 12, 20, 23, 31, 43]\n",
      "17 [143, 144, 141, 148, 167, 166]\n",
      "18 [30, 1, 51, 57, 2, 4]\n",
      "19 [89, 85, 86, 83, 36, 47]\n",
      "20 [122, 118, 131, 139, 114, 107]\n",
      "21 [13, 37, 33, 39, 90, 80]\n",
      "22 [169, 162, 159, 160, 165, 163]\n",
      "23 [120, 126, 133, 135, 116, 115]\n",
      "24 [87, 54, 98, 68, 88, 59]\n",
      "25 [127, 117, 137, 140, 129, 138]\n",
      "26 [100, 93, 46, 50, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Check out what the FewShotBatchSampler looks like\n",
    "\n",
    "print(\"Size of dataset:   \", len(labels))\n",
    "\n",
    "print(\"Classes:           \", fsb_s.classes)\n",
    "print(\"Number of classes: \", fsb_s.num_classes)\n",
    "print(\"Batches per class: \", fsb_s.batches_per_class)\n",
    "print(\"Indices per class: \", fsb_s.indices_per_class)\n",
    "print(\"Iterations:        \", fsb_s.iterations)\n",
    "print(\"Class list:        \", fsb_s.class_list)\n",
    "\n",
    "print(\"Print some batches\")\n",
    "for i, b in enumerate(fsb_s):\n",
    "    print(i, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices per class (0, 0): tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])\n",
      "Indices per class (0, 1): tensor([ 51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
      "         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
      "         93,  94,  95,  96,  97,  98,  99, 100, 101])\n",
      "Indices per class (1, 0): tensor([102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114])\n",
      "Indices per class (1, 1): tensor([115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127])\n",
      "Indices per class (1, 2): tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140])\n",
      "Indices per class (2, 0): tensor([141, 142, 143, 144, 145, 146, 147, 148, 149, 150])\n",
      "Indices per class (2, 1): tensor([151, 152, 153, 154, 155, 156, 157, 158, 159, 160])\n",
      "Indices per class (2, 2): tensor([161, 162, 163, 164, 165, 166, 167, 168, 169, 170])\n",
      "Task_list  (init):  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n",
      "Class_list (init):  {0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 1: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], 2: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]}\n",
      "Class_list (final):  {0: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], 1: [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2], 2: [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]}\n",
      "Task_list  (final):  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "tb_s = TaskBatchSampler(tasks, labels, 5, 3, 2, include_query=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: len=3, label=tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "Batch 1: len=3, label=tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 1, 1])\n",
      "Batch 2: len=3, label=tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "Batch 3: len=3, label=tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2,\n",
      "        0, 0, 1, 1, 2, 2])\n",
      "Batch 4: len=3, label=tensor([0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2,\n",
      "        0, 0, 1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "dl = data.DataLoader(\n",
    "    dataset, \n",
    "    batch_sampler=tb_s,\n",
    "    collate_fn=tb_s.get_collate_fn()\n",
    ")\n",
    "for i, (x, labels) in enumerate(dl):\n",
    "    print(\"Batch {}: len={}, label={}\".format(i, len(x), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices per class (0, 0): tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  24,  25,  30,  31,  32,\n",
      "         34,  35,  36,  37,  38,  39,  40,  44,  45,  46,  47,  48,  49,  50,\n",
      "         51,  52,  54,  55,  56,  68,  69,  70,  71,  72,  73,  74,  75,  77,\n",
      "         78,  80,  81,  82,  83,  85,  87,  88,  89,  90,  91,  92,  93,  94,\n",
      "         97,  98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 122, 136, 144, 157, 167, 172, 178, 184, 190,\n",
      "        195, 212, 213, 214, 215, 217, 220, 223, 225, 226, 231, 232, 241, 246,\n",
      "        247, 248, 249, 252, 256, 257, 263, 268, 269, 270, 272, 279, 280, 281,\n",
      "        284, 285, 288, 290, 291, 293, 295, 297, 298, 299, 301, 305])\n",
      "Indices per class (0, 1): tensor([ 23,  26,  27,  28,  29,  41,  42,  43,  53,  57,  58,  59,  60,  61,\n",
      "         62,  63,  64,  65,  66,  79,  99, 117, 118, 119, 120, 121, 123, 124,\n",
      "        125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140,\n",
      "        141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171,\n",
      "        173, 174, 175, 176, 177, 179, 180, 181, 182, 183, 185, 186, 187, 188,\n",
      "        189, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204,\n",
      "        205, 206, 207, 208, 209, 227, 228, 229, 230, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 242, 243, 244, 245, 250, 253, 254, 255, 259, 260, 261,\n",
      "        262, 264, 266, 267, 271, 273, 274, 276, 277, 278, 282, 283, 286, 287,\n",
      "        289, 292, 294, 296, 300, 303, 304])\n",
      "Indices per class (0, 2): tensor([ 33,  67,  76,  84,  86,  95,  96, 126, 210, 211, 216, 218, 219, 221,\n",
      "        222, 224, 251, 258, 265, 275, 302])\n",
      "Task_list  (init):  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Class_list (init):  {0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n",
      "Class_list (final):  {0: [0, 1, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 2, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 0, 1, 2, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 2, 0, 1, 1, 1, 2, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 2, 2, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1]}\n",
      "Task_list  (final):  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# See if this works\n",
    "# Testset is a list of tuple with (task, (input_ids, token_type_ids, attention_mask), label)\n",
    "\n",
    "dataset_tasks = torch.tensor([item[0] for item in test_set])\n",
    "dataset_targets = torch.tensor([item[2] for item in test_set])\n",
    "s = TaskBatchSampler(dataset_tasks, dataset_targets, 5, 3, 2, include_query=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 184 is out of bounds for dimension 0 with size 171",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m dl \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m      2\u001b[0m     dataset, \n\u001b[1;32m      3\u001b[0m     batch_sampler\u001b[38;5;241m=\u001b[39ms,\n\u001b[1;32m      4\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39ms\u001b[38;5;241m.\u001b[39mget_collate_fn()\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dl):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: len=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, label=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i, \u001b[38;5;28mlen\u001b[39m(x), labels))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/atcs_ml/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/atcs_ml/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/atcs_ml/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/atcs_ml/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mNLPDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 13\u001b[0m     task \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_ids[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_mask[idx])\n\u001b[1;32m     15\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 184 is out of bounds for dimension 0 with size 171"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO: \n",
    "THIS IS NOT WORKING YET\n",
    "PROBABLY BECAUSE OF PROBLEM WITH UNEQUAL CLASS SIZES WITHIN A TASK\n",
    "\"\"\"\n",
    "\n",
    "dl = data.DataLoader(\n",
    "    dataset, \n",
    "    batch_sampler=s,\n",
    "    collate_fn=s.get_collate_fn()\n",
    ")\n",
    "for i, (x, labels) in enumerate(dl):\n",
    "    print(\"Batch {}: len={}, label={}\".format(i, len(x), labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this sampler eventually allows the sampling of batches where we use a class twice due to a simpler shuffling function (`shuffle_data`). In other words, during training or validation, if we sample batches for a 5-class 4-shot training setting, it can happen that we get a batch where 2 of the 5 classes are identical. \n",
    "\n",
    "Since the NLP classification tasks only have a few classes, this happens very often. It does not constitute any issue if the code for the meta-learning methods support variable number of classes and shots per class. Nonetheless, for our NLP metalearner, we choose to set shuffle to False, and N-WAY to the max number of classes, so that each class is covered in each episode. \n",
    "\n",
    "For our experiments, we will use a training setting with 2/3-classes per task, and try out various options for K_SHOT, e.g. 4, 8, 16. This means that each support set contains 2/3 classes with 4/8/16 examples each, i.e., between 8-48 samples overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we implemented the sampling of a support and query set as sampling a support set with twice the number of examples. After sampling a batch from the data loader, we need to split it into a support and query set. We can summarize this step in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(inputs, targets):\n",
    "    # Split inputs and targets in two batches.\n",
    "    # Format needs to match with requirements of adaptorfusion model\n",
    "    \n",
    "    support_input_ids, query_input_ids = inputs[\"input_ids\"].chunk(2, dim=0)\n",
    "    support_token_type_ids, query_token_type_ids = inputs[\"token_type_ids\"].chunk(2, dim=0)\n",
    "    support_attention_mask, query_attention_mask = inputs[\"attention_mask\"].chunk(2, dim=0)\n",
    "    support_inputs = {\n",
    "        \"input_ids\": support_input_ids,\n",
    "        \"token_type_ids\": support_token_type_ids,\n",
    "        \"attention_mask\": support_attention_mask\n",
    "    } \n",
    "    query_inputs = {\n",
    "        \"input_ids\": query_input_ids,\n",
    "        \"token_type_ids\": query_token_type_ids,\n",
    "        \"attention_mask\": query_attention_mask\n",
    "    } \n",
    "    support_targets, query_targets = targets.chunk(2, dim=0)\n",
    "    return support_inputs, query_inputs, support_targets, query_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNet:\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_prototypes(features, targets):\n",
    "        # Given a stack of features vectors and labels, return class prototypes\n",
    "        # features - shape [N, proto_dim], targets - shape [N]\n",
    "        classes, _ = torch.unique(targets).sort()  # Determine which classes we have\n",
    "        prototypes = []\n",
    "        for c in classes:\n",
    "            p = features[torch.where(targets == c)[0]].mean(dim=0)  # Average class feature vectors\n",
    "            prototypes.append(p)\n",
    "        prototypes = torch.stack(prototypes, dim=0)\n",
    "        # Return the 'classes' tensor to know which prototype belongs to which class\n",
    "        return prototypes, classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAML and ProtoMAML\n",
    "\n",
    "The second meta-learning algorithm we will look at is MAML, short for Model-Agnostic Meta-Learning. MAML is an optimization-based meta-learning algorithm, which means that it tries to adjust the standard optimization procedure to a few-shot setting. The idea of MAML is relatively simple: given a model, support, and query set during training, we optimize the model for $m$ steps on the support set and evaluate the gradients of the query loss with respect to the original model's parameters. For the same model, we do it for a few different support-query sets and accumulate the gradients. This results in learning a model that provides a good initialization for being quickly adapted to the training tasks. If we denote the model parameters with $\\theta$, we can visualize the procedure as follows (Figure credit - [Finn et al.](http://proceedings.mlr.press/v70/finn17a.html)).\n",
    "\n",
    "\n",
    "The full algorithm of MAML is therefore as follows. At each training step, we sample a batch of tasks, i.e., a batch of support-query set pairs. For each task $\\mathcal{T}_i$, we optimize a model $f_{\\theta}$ on the support set via SGD, and denote this model as $f_{\\theta_i'}$. We refer to this optimization as _inner loop_. Using this new model, we calculate the gradients of the original parameters, $\\theta$, with respect to the query loss on $f_{\\theta_i'}$. These gradients are accumulated over all tasks and used to update $\\theta$. This is called _outer loop_ since we iterate over tasks. The full MAML algorithm is summarized below (Figure credit - [Finn et al.](http://proceedings.mlr.press/v70/finn17a.html)).\n",
    "\n",
    "To obtain gradients for the initial parameters $\\theta$ from the optimized model $f_{\\theta_i'}$, we actually need second-order gradients, i.e. gradients of gradients, as the support set gradients depend on $\\theta$ as well. This makes MAML computationally expensive, especially when using multiple inner loop steps. A simpler, yet almost equally well-performing alternative is First-Order MAML (FOMAML) which only uses first-order gradients. This means that the second-order gradients are ignored, and we can calculate the outer loop gradients (line 10 in algorithm 2) simply by calculating the gradients with respect to $\\theta_i'$ and use those as an update to $\\theta$. Hence, the new update rule becomes:\n",
    "\n",
    "$$\n",
    "\\theta\\leftarrow\\theta-\\beta\\sum_{\\mathcal{T}_i\\sim p(\\mathcal{T})}\\nabla_{\\theta_i'}\\mathcal{L}_{\\mathcal{T}_i}(f_{\\theta_i'})\n",
    "$$\n",
    "\n",
    "Note the change of $\\theta$ to $\\theta_i'$ for $\\nabla$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProtoMAML\n",
    "\n",
    "A problem of MAML is how to design the output classification layer. In case all tasks have a different number of classes, we need to initialize the output layer with zeros or randomly in every iteration. Even if we always have the same number of classes, we just start from random predictions. This requires several inner loop steps to reach a reasonable classification result. To overcome this problem, Triantafillou et al. (2020) propose to combine the merits of Prototypical Networks and MAML. Specifically, we can use prototypes to initialize our output layer to have a strong initialization. Thereby, it can be shown that the softmax over euclidean distances can be reformulated as a linear layer with softmax. To see this, let's first write out the negative Euclidean distance between a feature vector $f_{\\theta}(\\mathbf{x}^{*})$ of a new data point $\\mathbf{x}^{*}$ to a prototype $\\mathbf{v}_c$ of class $c$:\n",
    "\n",
    "$$\n",
    "-||f_{\\theta}(\\mathbf{x}^{*})-\\mathbf{v}_c||^2=-f_{\\theta}(\\mathbf{x}^{*})^Tf_{\\theta}(\\mathbf{x}^{*})+2\\mathbf{v}_c^{T}f_{\\theta}(\\mathbf{x}^{*})-\\mathbf{v}_c^T\\mathbf{v}_c\n",
    "$$\n",
    "\n",
    "We perform the classification across all classes $c\\in\\mathcal{C}$ and take a softmax on the distance. Hence, any term that is the same for all classes can be removed without changing the output probabilities. In the equation above, this is true for $-f_{\\theta}(\\mathbf{x}^{*})^Tf_{\\theta}(\\mathbf{x}^{*})$ since it is independent of any class prototype. Thus, we can write:\n",
    "\n",
    "$$\n",
    "-||f_{\\theta}(\\mathbf{x}^{*})-\\mathbf{v}_c||^2=2\\mathbf{v}_c^{T}f_{\\theta}(\\mathbf{x}^{*})-||\\mathbf{v}_c||^2+\\text{constant}\n",
    "$$\n",
    "\n",
    "Taking a second look at the equation above, it looks a lot like a linear layer. For this, we use $\\mathbf{W}_{c,\\cdot}=2\\mathbf{v}_c$ and $b_c=-||\\mathbf{v}_c||^2$ which gives us the linear layer $\\mathbf{W}f_{\\theta}(\\mathbf{x}^{*})+\\mathbf{b}$. Hence, if we initialize the output weight with twice the prototypes, and the biases by the negative squared L2 norm of the prototypes, we start with a Prototypical Network. MAML allows us to adapt this layer and the rest of the network further. \n",
    "\n",
    "In the following, we will implement First-Order ProtoMAML for few-shot classification. The implementation of MAML would be the same except for the output layer initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProtoMAML implementation\n",
    "\n",
    "For implementing ProtoMAML, we can follow Algorithm 2 with minor modifications. At each training step, we first sample a batch of tasks, and a support and query set for each task. In our case of few-shot classification, this means that we simply sample multiple support-query set pairs from our sampler. For each task, we finetune our current model on the support set. However, since we need to remember the original parameters for the other tasks, the outer loop gradient update, and future training steps, we need to create a copy of our model and finetune only the copy. We can copy a model by using standard Python functions like `deepcopy`. The inner loop is implemented in the function `adapt_few_shot` in the PyTorch Lightning module below. \n",
    "\n",
    "After finetuning the model, we apply it to the query set and calculate the first-order gradients with respect to the original parameters $\\theta$. In contrast to simple MAML, we also have to consider the gradients with respect to the output layer initialization, i.e. the prototypes, since they directly rely on $\\theta$. To realize this efficiently, we take two steps. First, we calculate the prototypes by applying the original model, i.e. not the copied model, on the support elements. When initializing the output layer, we detach the prototypes to stop the gradients. This is because, in the inner loop itself, we do not want to consider gradients through the prototypes back to the original model. However, after the inner loop is finished, we re-attach the computation graph of the prototypes by writing `output_weight = (output_weight - init_weight).detach() + init_weight`. While this line does not change the value of the variable `output_weight`, it adds its dependency on the prototype initialization `init_weight`. Thus, if we call `.backward` on `output_weight`, we will automatically calculate the first-order gradients with respect to the prototype initialization in the original model.\n",
    "\n",
    "After calculating all gradients and summing them together in the original model, we can take a standard optimizer step. PyTorch Lightning's method is however designed to return a loss-tensor on which we call `.backward` first. Since this is not possible here, we need to perform the optimization step ourselves. All details can be found in the code below.\n",
    "\n",
    "For implementing (Proto-)MAML with second-order gradients, it is recommended to use libraries such as [$\\nabla$higher](https://github.com/facebookresearch/higher) from Facebook AI Research. For simplicity, we stick with first-order methods here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock model for testing purposes --> Replace with adaptor fusion model\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "def get_transformer_model(output_size):\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=output_dim)\n",
    "    for param in model.bert.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoMAML(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, proto_dim, lr, lr_inner, lr_output, num_inner_steps):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            proto_dim - Dimensionality of prototype feature space\n",
    "            lr - Learning rate of the outer loop Adam optimizer\n",
    "            lr_inner - Learning rate of the inner loop SGD optimizer\n",
    "            lr_output - Learning rate for the output layer in the inner loop\n",
    "            num_inner_steps - Number of inner loop updates to perform\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = get_transformer_model(output_size=self.hparams.proto_dim)\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[140,180], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "        \n",
    "        \n",
    "    def run_model(self, local_model, output_weight, output_bias, inputs, labels):\n",
    "        \n",
    "        # Execute a model with given output layer weights and inputs\n",
    "        feats = local_model(inputs)\n",
    "        preds = F.linear(feats, output_weight, output_bias)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=1) == labels).float()\n",
    "        \n",
    "        return loss, preds, acc\n",
    "    \n",
    "    \n",
    "    def adapt_few_shot(self, support_inputs, support_targets):\n",
    "        \n",
    "        # Determine prototype initialization\n",
    "        support_feats = self.model(support_inputs)\n",
    "        prototypes, classes = ProtoNet.calculate_prototypes(support_feats, support_targets)\n",
    "        support_labels = (classes[None,:] == support_targets[:,None]).long().argmax(dim=-1)\n",
    "        \n",
    "        # Create inner-loop model and optimizer\n",
    "        local_model = deepcopy(self.model)\n",
    "        local_model.train()\n",
    "        local_optim = optim.SGD(local_model.parameters(), lr=self.hparams.lr_inner)\n",
    "        local_optim.zero_grad()\n",
    "        \n",
    "        # Create output layer weights with prototype-based initialization\n",
    "        init_weight = 2 * prototypes\n",
    "        init_bias = -torch.norm(prototypes, dim=1)**2\n",
    "        output_weight = init_weight.detach().requires_grad_()\n",
    "        output_bias = init_bias.detach().requires_grad_()\n",
    "        \n",
    "        # Optimize inner loop model on support set\n",
    "        for _ in range(self.hparams.num_inner_steps):\n",
    "            \n",
    "            # Determine loss on the support set\n",
    "            loss, _, _ = self.run_model(local_model, output_weight, output_bias, support_inputs, support_labels)\n",
    "            \n",
    "            # Calculate gradients and perform inner loop update\n",
    "            loss.backward()\n",
    "            local_optim.step()\n",
    "            \n",
    "            # Update output layer via SGD\n",
    "            output_weight.data -= self.hparams.lr_output * output_weight.grad\n",
    "            output_bias.data -= self.hparams.lr_output * output_bias.grad\n",
    "            \n",
    "            # Reset gradients\n",
    "            local_optim.zero_grad()\n",
    "            output_weight.grad.fill_(0)\n",
    "            output_bias.grad.fill_(0)\n",
    "            \n",
    "        # Re-attach computation graph of prototypes\n",
    "        output_weight = (output_weight - init_weight).detach() + init_weight\n",
    "        output_bias = (output_bias - init_bias).detach() + init_bias\n",
    "        \n",
    "        return local_model, output_weight, output_bias, classes\n",
    "\n",
    "    \n",
    "    def outer_loop(self, batch, mode=\"train\"):\n",
    "\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Determine gradients for batch of tasks\n",
    "        for task_batch in batch:\n",
    "            inputs, targets = task_batch\n",
    "            support_inputs, query_inputs, support_targets, query_targets = split_batch(inputs, targets)\n",
    "            \n",
    "            # Perform inner loop adaptation\n",
    "            local_model, output_weight, output_bias, classes = self.adapt_few_shot(support_inputs, support_targets)\n",
    "            \n",
    "            # Determine loss of query set\n",
    "            query_labels = (classes[None,:] == query_targets[:,None]).long().argmax(dim=-1)\n",
    "            loss, preds, acc = self.run_model(local_model, output_weight, output_bias, query_inputs, query_labels)\n",
    "            \n",
    "            # Calculate gradients for query set loss\n",
    "            if mode == \"train\":\n",
    "                loss.backward()\n",
    "\n",
    "                for p_global, p_local in zip(self.model.parameters(), local_model.parameters()):\n",
    "                    \n",
    "                    # First-order approx. -> add gradients of finetuned and base model\n",
    "                    p_global.grad += p_local.grad  \n",
    "            \n",
    "            accuracies.append(acc.mean().detach())\n",
    "            losses.append(loss.detach())\n",
    "        \n",
    "        # Perform update of base model\n",
    "        if mode == \"train\":\n",
    "            opt = self.optimizers()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        self.log(f\"{mode}_loss\", sum(losses) / len(losses))\n",
    "        self.log(f\"{mode}_acc\", sum(accuracies) / len(accuracies))\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.outer_loop(batch, mode=\"train\")\n",
    "        return None  # Returning None means we skip the default training optimizer steps by PyTorch Lightning\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Validation requires to finetune a model, hence we need to enable gradients\n",
    "        torch.set_grad_enabled(True)\n",
    "        self.outer_loop(batch, mode=\"val\")\n",
    "        torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To train ProtoMAML, we need to change our sampling slightly. Instead of a single support-query set batch, we need to sample multiple. To implement this, we yet use another Sampler that combines multiple batches from a `FewShotBatchSampler` and returns it afterward. Additionally, we define a `collate_fn` for our data loader which takes the stack of support-query set images and returns the tasks as a list. This makes it easier to process in our PyTorch Lightning module before. The implementation of the sampler can be found below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creation of the data loaders is with this sampler straight-forward. Note that since many samples need to loaded for a training batch, it is recommended to use less workers than usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_class, train_loader, val_loader, **kwargs):\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, model_class.__name__),\n",
    "        gpus=1 if torch.cuda.is_available() else 0,\n",
    "        max_epochs=200,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "            LearningRateMonitor(\"epoch\")\n",
    "        ],\n",
    "        progress_bar_refresh_rate=0)\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, model_class.__name__ + \".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        # Automatically loads the model with the saved hyperparameters\n",
    "        model = model_class.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)  # To be reproducable\n",
    "        model = model_class(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        # Load best checkpoint after training\n",
    "        model = model_class.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)  \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices per class (0, 0): tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  24,  25,  30,  31,  32,\n",
      "         34,  35,  36,  37,  38,  39,  40,  44,  45,  46,  47,  48,  49,  50,\n",
      "         51,  52,  54,  55,  56,  68,  69,  70,  71,  72,  73,  74,  75,  77,\n",
      "         78,  80,  81,  82,  83,  85,  87,  88,  89,  90,  91,  92,  93,  94,\n",
      "         97,  98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 122, 136, 144, 157, 167, 172, 178, 184, 190,\n",
      "        195, 212, 213, 214, 215, 217, 220, 223, 225, 226, 231, 232, 241, 246,\n",
      "        247, 248, 249, 252, 256, 257, 263, 268, 269, 270, 272, 279, 280, 281,\n",
      "        284, 285, 288, 290, 291, 293, 295, 297, 298, 299, 301, 305])\n",
      "Indices per class (0, 1): tensor([ 23,  26,  27,  28,  29,  41,  42,  43,  53,  57,  58,  59,  60,  61,\n",
      "         62,  63,  64,  65,  66,  79,  99, 117, 118, 119, 120, 121, 123, 124,\n",
      "        125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140,\n",
      "        141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171,\n",
      "        173, 174, 175, 176, 177, 179, 180, 181, 182, 183, 185, 186, 187, 188,\n",
      "        189, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204,\n",
      "        205, 206, 207, 208, 209, 227, 228, 229, 230, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 242, 243, 244, 245, 250, 253, 254, 255, 259, 260, 261,\n",
      "        262, 264, 266, 267, 271, 273, 274, 276, 277, 278, 282, 283, 286, 287,\n",
      "        289, 292, 294, 296, 300, 303, 304])\n",
      "Indices per class (0, 2): tensor([ 33,  67,  76,  84,  86,  95,  96, 126, 210, 211, 216, 218, 219, 221,\n",
      "        222, 224, 251, 258, 265, 275, 302])\n",
      "Task_list  (init):  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Class_list (init):  {0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2]}\n",
      "Class_list (final):  {0: [0, 1, 2, 0, 1, 2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1]}\n",
      "Task_list  (final):  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Indices per class (0, 0): tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  24,  25,  30,  31,  32,\n",
      "         34,  35,  36,  37,  38,  39,  40,  44,  45,  46,  47,  48,  49,  50,\n",
      "         51,  52,  54,  55,  56,  68,  69,  70,  71,  72,  73,  74,  75,  77,\n",
      "         78,  80,  81,  82,  83,  85,  87,  88,  89,  90,  91,  92,  93,  94,\n",
      "         97,  98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 122, 136, 144, 157, 167, 172, 178, 184, 190,\n",
      "        195, 212, 213, 214, 215, 217, 220, 223, 225, 226, 231, 232, 241, 246,\n",
      "        247, 248, 249, 252, 256, 257, 263, 268, 269, 270, 272, 279, 280, 281,\n",
      "        284, 285, 288, 290, 291, 293, 295, 297, 298, 299, 301, 305])\n",
      "Indices per class (0, 1): tensor([ 23,  26,  27,  28,  29,  41,  42,  43,  53,  57,  58,  59,  60,  61,\n",
      "         62,  63,  64,  65,  66,  79,  99, 117, 118, 119, 120, 121, 123, 124,\n",
      "        125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140,\n",
      "        141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171,\n",
      "        173, 174, 175, 176, 177, 179, 180, 181, 182, 183, 185, 186, 187, 188,\n",
      "        189, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204,\n",
      "        205, 206, 207, 208, 209, 227, 228, 229, 230, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 242, 243, 244, 245, 250, 253, 254, 255, 259, 260, 261,\n",
      "        262, 264, 266, 267, 271, 273, 274, 276, 277, 278, 282, 283, 286, 287,\n",
      "        289, 292, 294, 296, 300, 303, 304])\n",
      "Indices per class (0, 2): tensor([ 33,  67,  76,  84,  86,  95,  96, 126, 210, 211, 216, 218, 219, 221,\n",
      "        222, 224, 251, 258, 265, 275, 302])\n",
      "Task_list  (init):  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Class_list (init):  {0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2]}\n",
      "Class_list (final):  {0: [0, 1, 2, 0, 1, 2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1]}\n",
      "Task_list  (final):  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Training constant\n",
    "N_WAY = 3     # All tasks have 2 or 3 classes, so set to 3 to ensure all classes are covered in an episode\n",
    "K_SHOT = 4\n",
    "\n",
    "# Training set\n",
    "train_protomaml_sampler = TaskBatchSampler(\n",
    "    train_set.tasks,\n",
    "    train_set.labels, \n",
    "    include_query=True,\n",
    "    N_way=N_WAY,\n",
    "    K_shot=K_SHOT,\n",
    "    batch_size=16,\n",
    "    shuffle=False   # Set to False, otherwise you risk getting same class twice in dataset\n",
    ")\n",
    "train_protomaml_loader = data.DataLoader(\n",
    "    train_set, \n",
    "    batch_sampler=train_protomaml_sampler,\n",
    "    collate_fn=train_protomaml_sampler.get_collate_fn(),\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Validation set\n",
    "val_protomaml_sampler = TaskBatchSampler(\n",
    "    val_set.tasks,\n",
    "    val_set.labels,\n",
    "    include_query=True,\n",
    "    N_way=N_WAY,\n",
    "    K_shot=K_SHOT,\n",
    "    batch_size=1,  # We do not update the parameters, hence the batch size is irrelevant here\n",
    "    shuffle=False\n",
    ")\n",
    "val_protomaml_loader = data.DataLoader(\n",
    "    val_set, \n",
    "    batch_sampler=val_protomaml_sampler,\n",
    "    collate_fn=val_protomaml_sampler.get_collate_fn(),\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to train our ProtoMAML. We use the same feature space size as for ProtoNet but can use a higher learning rate since the outer loop gradients are accumulated over 16 batches. The inner loop learning rate is set to 0.1, which is much higher than the outer loop learning rate because we use SGD in the inner loop instead of Adam. Commonly, the learning rate for the output layer is higher than the base model if the base model is very deep or pre-trained. However, for our setup, we observed no noticeable impact of using a different learning rate than the base model. The number of inner loop updates is another crucial hyperparameter and depends on the similarity of our training tasks. Since all tasks are on images from the same dataset, we notice that a single inner loop update achieves similar performance as 3 or 5 while training considerably faster. However, especially in RL and NLP, a larger number of inner loop steps are often needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "protomaml_model = train_model(\n",
    "    ProtoMAML, \n",
    "    proto_dim=64, \n",
    "    lr=1e-3, \n",
    "    lr_inner=0.1,\n",
    "    lr_output=0.1,\n",
    "    num_inner_steps=1,  # Often values between 1 and 10\n",
    "    train_loader=train_protomaml_loader, \n",
    "    val_loader=val_protomaml_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "We test ProtoMAML in the same manner as ProtoNet, namely by picking random examples in the test set as support sets and use the rest of the dataset as the query set. Instead of just calculating the prototypes for all examples, we need to finetune a separate model for each support set. This is why this process is more expensive than ProtoNet, and in our case, testing $k=\\{2,4,8,16,32\\}$ can take almost an hour. Hence, we provide evaluation files besides the pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_protomaml(model, dataset, k_shot=4):\n",
    "    \n",
    "    pl.seed_everything(42)\n",
    "    model = model.to(device)\n",
    "    num_classes = dataset.targets.unique().shape[0]\n",
    "    exmps_per_class = dataset.targets.shape[0]//num_classes\n",
    "    \n",
    "    # Data loader for full test set as query set\n",
    "    full_dataloader = data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=128, \n",
    "        num_workers=4, \n",
    "        shuffle=False, \n",
    "        drop_last=False\n",
    "    )\n",
    "    # Data loader for sampling support sets\n",
    "    sampler = FewShotBatchSampler(\n",
    "        dataset.targets, \n",
    "        include_query=False,\n",
    "        N_way=num_classes,\n",
    "        K_shot=k_shot,\n",
    "        shuffle=False,\n",
    "        shuffle_once=False\n",
    "    )\n",
    "    sample_dataloader = data.DataLoader(dataset, batch_sampler=sampler, num_workers=2)\n",
    "    \n",
    "    # We iterate through the full dataset in two manners. First, to select the k-shot batch. \n",
    "    # Second, the evaluate the model on all other examples\n",
    "    accuracies = []\n",
    "    for (support_imgs, support_targets), support_indices in tqdm(zip(sample_dataloader, sampler), \"Performing few-shot finetuning\"):\n",
    "        support_imgs = support_imgs.to(device)\n",
    "        support_targets = support_targets.to(device)\n",
    "        \n",
    "        # Finetune new model on support set\n",
    "        local_model, output_weight, output_bias, classes = model.adapt_few_shot(support_imgs, support_targets)\n",
    "        with torch.no_grad():  # No gradients for query set needed\n",
    "            local_model.eval()\n",
    "            batch_acc = torch.zeros((0,), dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Evaluate all examples in test dataset\n",
    "            for query_imgs, query_targets in full_dataloader:\n",
    "                query_imgs = query_imgs.to(device)\n",
    "                query_targets = query_targets.to(device)\n",
    "                query_labels = (classes[None,:] == query_targets[:,None]).long().argmax(dim=-1)\n",
    "                _, _, acc = model.run_model(local_model, output_weight, output_bias, query_imgs, query_labels)\n",
    "                batch_acc = torch.cat([batch_acc, acc.detach()], dim=0)\n",
    "                \n",
    "            # Exclude support set elements\n",
    "            for s_idx in support_indices:\n",
    "                batch_acc[s_idx] = 0\n",
    "            batch_acc = batch_acc.sum().item() / (batch_acc.shape[0] - len(support_indices))\n",
    "            accuracies.append(batch_acc)\n",
    "            \n",
    "    return mean(accuracies), stdev(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to training, it is recommended to use many more inner loop updates during testing. During training, we are not interested in getting the best model from the inner loop, but the model which can provide the best gradients. Hence, one update might be already sufficient in training, but for testing, it was often observed that a larger number of updates can give a considerable performance boost. Thus, we change the inner loop updates to 200 before testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protomaml_model.hparams.num_inner_steps = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test our model. For the pre-trained models, we provide a json file with the results to reduce evaluation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protomaml_result_file = os.path.join(CHECKPOINT_PATH, \"protomaml_fewshot.json\")\n",
    "\n",
    "if os.path.isfile(protomaml_result_file):\n",
    "    # Load pre-computed results\n",
    "    with open(protomaml_result_file, 'r') as f:\n",
    "        protomaml_accuracies = json.load(f)\n",
    "    protomaml_accuracies = {int(k): v for k, v in protomaml_accuracies.items()}\n",
    "else:\n",
    "    # Perform experiments\n",
    "    protomaml_accuracies = dict()\n",
    "    for k in [2, 4, 8, 16, 32]:\n",
    "        protomaml_accuracies[k] = test_protomaml(protomaml_model, test_set, k_shot=k)\n",
    "    # Export results\n",
    "    with open(protomaml_result_file, 'w') as f:\n",
    "        json.dump(protomaml_accuracies, f, indent=4)\n",
    "\n",
    "for k in protomaml_accuracies:\n",
    "    print(f\"Accuracy for k={k}: {100.0*protomaml_accuracies[k][0]:4.2f}% (+-{100.0*protomaml_accuracies[k][1]:4.2f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

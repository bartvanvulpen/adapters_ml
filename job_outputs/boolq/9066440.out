Reusing dataset super_glue (/home/lcur1216/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)
  0%|          | 0/3 [00:00<?, ?it/s]100%|##########| 3/3 [00:00<00:00, 412.51it/s]
Loading cached processed dataset at /home/lcur1216/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-5d9e1b59b8623faf.arrow
Loading cached processed dataset at /home/lcur1216/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-ae3ee6c302918c67.arrow
Loading cached processed dataset at /home/lcur1216/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-4dd8c6b46844053e.arrow
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/transformers/adapters/models/bert.py:245: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.
  warnings.warn(
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/transformers/adapters/models/bert.py:223: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.
  warnings.warn(
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 9427
  Num Epochs = 10
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 5900
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 3270
  Batch size = 16
Saving model checkpoint to training_output/boolq/checkpoints/checkpoint-590
Configuration saved in training_output/boolq/checkpoints/checkpoint-590/multinli/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-590/multinli/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-590/qqp/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-590/qqp/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-590/sst/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-590/sst/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-590/wgrande/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-590/wgrande/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-590/boolq/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-590/boolq/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-590/multinli,qqp,sst,wgrande,boolq/adapter_fusion_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-590/multinli,qqp,sst,wgrande,boolq/pytorch_model_adapter_fusion.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-590/boolq_classifier/head_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-590/boolq_classifier/pytorch_model_head.bin
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 3270
  Batch size = 16
Saving model checkpoint to training_output/boolq/checkpoints/checkpoint-1180
Configuration saved in training_output/boolq/checkpoints/checkpoint-1180/multinli/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1180/multinli/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-1180/qqp/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1180/qqp/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-1180/sst/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1180/sst/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-1180/wgrande/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1180/wgrande/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-1180/boolq/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1180/boolq/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-1180/multinli,qqp,sst,wgrande,boolq/adapter_fusion_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1180/multinli,qqp,sst,wgrande,boolq/pytorch_model_adapter_fusion.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-1180/boolq_classifier/head_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1180/boolq_classifier/pytorch_model_head.bin
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 3270
  Batch size = 16
Saving model checkpoint to training_output/boolq/checkpoints/checkpoint-1770
Configuration saved in training_output/boolq/checkpoints/checkpoint-1770/multinli/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1770/multinli/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-1770/qqp/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1770/qqp/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-1770/sst/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1770/sst/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-1770/wgrande/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1770/wgrande/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-1770/boolq/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1770/boolq/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-1770/multinli,qqp,sst,wgrande,boolq/adapter_fusion_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1770/multinli,qqp,sst,wgrande,boolq/pytorch_model_adapter_fusion.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-1770/boolq_classifier/head_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-1770/boolq_classifier/pytorch_model_head.bin
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 3270
  Batch size = 16
Saving model checkpoint to training_output/boolq/checkpoints/checkpoint-2360
Configuration saved in training_output/boolq/checkpoints/checkpoint-2360/multinli/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2360/multinli/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-2360/qqp/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2360/qqp/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-2360/sst/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2360/sst/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-2360/wgrande/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2360/wgrande/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-2360/boolq/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2360/boolq/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-2360/multinli,qqp,sst,wgrande,boolq/adapter_fusion_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2360/multinli,qqp,sst,wgrande,boolq/pytorch_model_adapter_fusion.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-2360/boolq_classifier/head_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2360/boolq_classifier/pytorch_model_head.bin
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 3270
  Batch size = 16
Saving model checkpoint to training_output/boolq/checkpoints/checkpoint-2950
Configuration saved in training_output/boolq/checkpoints/checkpoint-2950/multinli/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2950/multinli/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-2950/qqp/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2950/qqp/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-2950/sst/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2950/sst/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-2950/wgrande/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2950/wgrande/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-2950/boolq/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2950/boolq/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-2950/multinli,qqp,sst,wgrande,boolq/adapter_fusion_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2950/multinli,qqp,sst,wgrande,boolq/pytorch_model_adapter_fusion.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-2950/boolq_classifier/head_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-2950/boolq_classifier/pytorch_model_head.bin
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 3270
  Batch size = 16
Saving model checkpoint to training_output/boolq/checkpoints/checkpoint-3540
Configuration saved in training_output/boolq/checkpoints/checkpoint-3540/multinli/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-3540/multinli/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-3540/qqp/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-3540/qqp/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-3540/sst/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-3540/sst/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-3540/wgrande/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-3540/wgrande/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-3540/boolq/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-3540/boolq/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-3540/multinli,qqp,sst,wgrande,boolq/adapter_fusion_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-3540/multinli,qqp,sst,wgrande,boolq/pytorch_model_adapter_fusion.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-3540/boolq_classifier/head_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-3540/boolq_classifier/pytorch_model_head.bin
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 3270
  Batch size = 16
Saving model checkpoint to training_output/boolq/checkpoints/checkpoint-4130
Configuration saved in training_output/boolq/checkpoints/checkpoint-4130/multinli/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4130/multinli/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-4130/qqp/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4130/qqp/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-4130/sst/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4130/sst/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-4130/wgrande/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4130/wgrande/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-4130/boolq/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4130/boolq/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-4130/multinli,qqp,sst,wgrande,boolq/adapter_fusion_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4130/multinli,qqp,sst,wgrande,boolq/pytorch_model_adapter_fusion.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-4130/boolq_classifier/head_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4130/boolq_classifier/pytorch_model_head.bin
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 3270
  Batch size = 16
Saving model checkpoint to training_output/boolq/checkpoints/checkpoint-4720
Configuration saved in training_output/boolq/checkpoints/checkpoint-4720/multinli/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4720/multinli/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-4720/qqp/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4720/qqp/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-4720/sst/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4720/sst/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-4720/wgrande/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4720/wgrande/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-4720/boolq/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4720/boolq/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-4720/multinli,qqp,sst,wgrande,boolq/adapter_fusion_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4720/multinli,qqp,sst,wgrande,boolq/pytorch_model_adapter_fusion.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-4720/boolq_classifier/head_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-4720/boolq_classifier/pytorch_model_head.bin
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 3270
  Batch size = 16
Saving model checkpoint to training_output/boolq/checkpoints/checkpoint-5310
Configuration saved in training_output/boolq/checkpoints/checkpoint-5310/multinli/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5310/multinli/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-5310/qqp/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5310/qqp/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-5310/sst/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5310/sst/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-5310/wgrande/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5310/wgrande/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-5310/boolq/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5310/boolq/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-5310/multinli,qqp,sst,wgrande,boolq/adapter_fusion_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5310/multinli,qqp,sst,wgrande,boolq/pytorch_model_adapter_fusion.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-5310/boolq_classifier/head_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5310/boolq_classifier/pytorch_model_head.bin
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 3270
  Batch size = 16
Saving model checkpoint to training_output/boolq/checkpoints/checkpoint-5900
Configuration saved in training_output/boolq/checkpoints/checkpoint-5900/multinli/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5900/multinli/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-5900/qqp/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5900/qqp/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-5900/sst/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5900/sst/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-5900/wgrande/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5900/wgrande/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-5900/boolq/adapter_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5900/boolq/pytorch_adapter.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-5900/multinli,qqp,sst,wgrande,boolq/adapter_fusion_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5900/multinli,qqp,sst,wgrande,boolq/pytorch_model_adapter_fusion.bin
Configuration saved in training_output/boolq/checkpoints/checkpoint-5900/boolq_classifier/head_config.json
Module weights saved in training_output/boolq/checkpoints/checkpoint-5900/boolq_classifier/pytorch_model_head.bin


Training completed. Do not forget to share your model on huggingface.co/models =)


***** Running Evaluation *****
  Num examples = 3270
  Batch size = 16
/home/lcur1216/.conda/envs/ada_gpu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Configuration saved in saved/fusion/boolq/adapter_fusion_config.json
Module weights saved in saved/fusion/boolq/pytorch_model_adapter_fusion.bin
Configuration saved in saved/sep_adapters/boolq/multinli/adapter_config.json
Module weights saved in saved/sep_adapters/boolq/multinli/pytorch_adapter.bin
Configuration saved in saved/sep_adapters/boolq/qqp/adapter_config.json
Module weights saved in saved/sep_adapters/boolq/qqp/pytorch_adapter.bin
Configuration saved in saved/sep_adapters/boolq/sst/adapter_config.json
Module weights saved in saved/sep_adapters/boolq/sst/pytorch_adapter.bin
Configuration saved in saved/sep_adapters/boolq/wgrande/adapter_config.json
Module weights saved in saved/sep_adapters/boolq/wgrande/pytorch_adapter.bin
Configuration saved in saved/sep_adapters/boolq/boolq/adapter_config.json
Module weights saved in saved/sep_adapters/boolq/boolq/pytorch_adapter.bin
{'loss': 0.6573, 'learning_rate': 5e-05, 'epoch': 0.08}
{'loss': 0.6275, 'learning_rate': 5e-05, 'epoch': 0.17}
{'loss': 0.593, 'learning_rate': 5e-05, 'epoch': 0.25}
{'loss': 0.5059, 'learning_rate': 5e-05, 'epoch': 0.34}
{'loss': 0.4998, 'learning_rate': 5e-05, 'epoch': 0.42}
{'loss': 0.4666, 'learning_rate': 5e-05, 'epoch': 0.51}
{'loss': 0.4696, 'learning_rate': 5e-05, 'epoch': 0.59}
{'loss': 0.4721, 'learning_rate': 5e-05, 'epoch': 0.68}
{'loss': 0.457, 'learning_rate': 5e-05, 'epoch': 0.76}
{'loss': 0.4655, 'learning_rate': 5e-05, 'epoch': 0.85}
{'loss': 0.4487, 'learning_rate': 5e-05, 'epoch': 0.93}
{'eval_loss': 0.6555472016334534, 'eval_acc': 0.7070336391437309, 'eval_runtime': 35.6429, 'eval_samples_per_second': 91.743, 'eval_steps_per_second': 5.751, 'epoch': 1.0}
{'loss': 0.4001, 'learning_rate': 5e-05, 'epoch': 1.02}
{'loss': 0.391, 'learning_rate': 5e-05, 'epoch': 1.1}
{'loss': 0.3804, 'learning_rate': 5e-05, 'epoch': 1.19}
{'loss': 0.3715, 'learning_rate': 5e-05, 'epoch': 1.27}
{'loss': 0.3906, 'learning_rate': 5e-05, 'epoch': 1.36}
{'loss': 0.3717, 'learning_rate': 5e-05, 'epoch': 1.44}
{'loss': 0.3901, 'learning_rate': 5e-05, 'epoch': 1.53}
{'loss': 0.4002, 'learning_rate': 5e-05, 'epoch': 1.61}
{'loss': 0.3641, 'learning_rate': 5e-05, 'epoch': 1.69}
{'loss': 0.3708, 'learning_rate': 5e-05, 'epoch': 1.78}
{'loss': 0.3792, 'learning_rate': 5e-05, 'epoch': 1.86}
{'loss': 0.3877, 'learning_rate': 5e-05, 'epoch': 1.95}
{'eval_loss': 0.6990875005722046, 'eval_acc': 0.7159021406727829, 'eval_runtime': 35.5721, 'eval_samples_per_second': 91.926, 'eval_steps_per_second': 5.763, 'epoch': 2.0}
{'loss': 0.3474, 'learning_rate': 5e-05, 'epoch': 2.03}
{'loss': 0.3483, 'learning_rate': 5e-05, 'epoch': 2.12}
{'loss': 0.3299, 'learning_rate': 5e-05, 'epoch': 2.2}
{'loss': 0.3419, 'learning_rate': 5e-05, 'epoch': 2.29}
{'loss': 0.33, 'learning_rate': 5e-05, 'epoch': 2.37}
{'loss': 0.3528, 'learning_rate': 5e-05, 'epoch': 2.46}
{'loss': 0.3635, 'learning_rate': 5e-05, 'epoch': 2.54}
{'loss': 0.3379, 'learning_rate': 5e-05, 'epoch': 2.63}
{'loss': 0.3363, 'learning_rate': 5e-05, 'epoch': 2.71}
{'loss': 0.3456, 'learning_rate': 5e-05, 'epoch': 2.8}
{'loss': 0.4015, 'learning_rate': 5e-05, 'epoch': 2.88}
{'loss': 0.3878, 'learning_rate': 5e-05, 'epoch': 2.97}
{'eval_loss': 0.7540158629417419, 'eval_acc': 0.717737003058104, 'eval_runtime': 35.6173, 'eval_samples_per_second': 91.809, 'eval_steps_per_second': 5.756, 'epoch': 3.0}
{'loss': 0.2869, 'learning_rate': 5e-05, 'epoch': 3.05}
{'loss': 0.3329, 'learning_rate': 5e-05, 'epoch': 3.14}
{'loss': 0.3415, 'learning_rate': 5e-05, 'epoch': 3.22}
{'loss': 0.3563, 'learning_rate': 5e-05, 'epoch': 3.31}
{'loss': 0.3628, 'learning_rate': 5e-05, 'epoch': 3.39}
{'loss': 0.3577, 'learning_rate': 5e-05, 'epoch': 3.47}
{'loss': 0.3197, 'learning_rate': 5e-05, 'epoch': 3.56}
{'loss': 0.3474, 'learning_rate': 5e-05, 'epoch': 3.64}
{'loss': 0.3601, 'learning_rate': 5e-05, 'epoch': 3.73}
{'loss': 0.3378, 'learning_rate': 5e-05, 'epoch': 3.81}
{'loss': 0.3486, 'learning_rate': 5e-05, 'epoch': 3.9}
{'loss': 0.3345, 'learning_rate': 5e-05, 'epoch': 3.98}
{'eval_loss': 0.7423117160797119, 'eval_acc': 0.7299694189602447, 'eval_runtime': 35.7397, 'eval_samples_per_second': 91.495, 'eval_steps_per_second': 5.736, 'epoch': 4.0}
{'loss': 0.3224, 'learning_rate': 5e-05, 'epoch': 4.07}
{'loss': 0.3305, 'learning_rate': 5e-05, 'epoch': 4.15}
{'loss': 0.3421, 'learning_rate': 5e-05, 'epoch': 4.24}
{'loss': 0.3101, 'learning_rate': 5e-05, 'epoch': 4.32}
{'loss': 0.3066, 'learning_rate': 5e-05, 'epoch': 4.41}
{'loss': 0.3081, 'learning_rate': 5e-05, 'epoch': 4.49}
{'loss': 0.331, 'learning_rate': 5e-05, 'epoch': 4.58}
{'loss': 0.3669, 'learning_rate': 5e-05, 'epoch': 4.66}
{'loss': 0.3725, 'learning_rate': 5e-05, 'epoch': 4.75}
{'loss': 0.3205, 'learning_rate': 5e-05, 'epoch': 4.83}
{'loss': 0.3252, 'learning_rate': 5e-05, 'epoch': 4.92}
{'loss': 0.3439, 'learning_rate': 5e-05, 'epoch': 5.0}
{'eval_loss': 0.7155619859695435, 'eval_acc': 0.7327217125382263, 'eval_runtime': 35.6617, 'eval_samples_per_second': 91.695, 'eval_steps_per_second': 5.748, 'epoch': 5.0}
{'loss': 0.2905, 'learning_rate': 5e-05, 'epoch': 5.08}
{'loss': 0.3075, 'learning_rate': 5e-05, 'epoch': 5.17}
{'loss': 0.3538, 'learning_rate': 5e-05, 'epoch': 5.25}
{'loss': 0.3518, 'learning_rate': 5e-05, 'epoch': 5.34}
{'loss': 0.3164, 'learning_rate': 5e-05, 'epoch': 5.42}
{'loss': 0.291, 'learning_rate': 5e-05, 'epoch': 5.51}
{'loss': 0.3538, 'learning_rate': 5e-05, 'epoch': 5.59}
{'loss': 0.3351, 'learning_rate': 5e-05, 'epoch': 5.68}
{'loss': 0.3337, 'learning_rate': 5e-05, 'epoch': 5.76}
{'loss': 0.3524, 'learning_rate': 5e-05, 'epoch': 5.85}
{'loss': 0.324, 'learning_rate': 5e-05, 'epoch': 5.93}
{'eval_loss': 0.7223950624465942, 'eval_acc': 0.7269113149847095, 'eval_runtime': 35.6857, 'eval_samples_per_second': 91.633, 'eval_steps_per_second': 5.745, 'epoch': 6.0}
{'loss': 0.3013, 'learning_rate': 5e-05, 'epoch': 6.02}
{'loss': 0.3147, 'learning_rate': 5e-05, 'epoch': 6.1}
{'loss': 0.3012, 'learning_rate': 5e-05, 'epoch': 6.19}
{'loss': 0.2885, 'learning_rate': 5e-05, 'epoch': 6.27}
{'loss': 0.317, 'learning_rate': 5e-05, 'epoch': 6.36}
{'loss': 0.3027, 'learning_rate': 5e-05, 'epoch': 6.44}
{'loss': 0.3408, 'learning_rate': 5e-05, 'epoch': 6.53}
{'loss': 0.3484, 'learning_rate': 5e-05, 'epoch': 6.61}
{'loss': 0.3192, 'learning_rate': 5e-05, 'epoch': 6.69}
{'loss': 0.3221, 'learning_rate': 5e-05, 'epoch': 6.78}
{'loss': 0.3448, 'learning_rate': 5e-05, 'epoch': 6.86}
{'loss': 0.3381, 'learning_rate': 5e-05, 'epoch': 6.95}
{'eval_loss': 0.7845461368560791, 'eval_acc': 0.7253822629969419, 'eval_runtime': 35.7824, 'eval_samples_per_second': 91.386, 'eval_steps_per_second': 5.729, 'epoch': 7.0}
{'loss': 0.3301, 'learning_rate': 5e-05, 'epoch': 7.03}
{'loss': 0.2862, 'learning_rate': 5e-05, 'epoch': 7.12}
{'loss': 0.3337, 'learning_rate': 5e-05, 'epoch': 7.2}
{'loss': 0.3221, 'learning_rate': 5e-05, 'epoch': 7.29}
{'loss': 0.2991, 'learning_rate': 5e-05, 'epoch': 7.37}
{'loss': 0.3241, 'learning_rate': 5e-05, 'epoch': 7.46}
{'loss': 0.3399, 'learning_rate': 5e-05, 'epoch': 7.54}
{'loss': 0.3352, 'learning_rate': 5e-05, 'epoch': 7.63}
{'loss': 0.3241, 'learning_rate': 5e-05, 'epoch': 7.71}
{'loss': 0.3442, 'learning_rate': 5e-05, 'epoch': 7.8}
{'loss': 0.3435, 'learning_rate': 5e-05, 'epoch': 7.88}
{'loss': 0.3525, 'learning_rate': 5e-05, 'epoch': 7.97}
{'eval_loss': 0.7166851758956909, 'eval_acc': 0.7269113149847095, 'eval_runtime': 35.8856, 'eval_samples_per_second': 91.123, 'eval_steps_per_second': 5.713, 'epoch': 8.0}
{'loss': 0.2875, 'learning_rate': 5e-05, 'epoch': 8.05}
{'loss': 0.2947, 'learning_rate': 5e-05, 'epoch': 8.14}
{'loss': 0.3309, 'learning_rate': 5e-05, 'epoch': 8.22}
{'loss': 0.3053, 'learning_rate': 5e-05, 'epoch': 8.31}
{'loss': 0.3469, 'learning_rate': 5e-05, 'epoch': 8.39}
{'loss': 0.3311, 'learning_rate': 5e-05, 'epoch': 8.47}
{'loss': 0.3368, 'learning_rate': 5e-05, 'epoch': 8.56}
{'loss': 0.3438, 'learning_rate': 5e-05, 'epoch': 8.64}
{'loss': 0.3316, 'learning_rate': 5e-05, 'epoch': 8.73}
{'loss': 0.3246, 'learning_rate': 5e-05, 'epoch': 8.81}
{'loss': 0.3084, 'learning_rate': 5e-05, 'epoch': 8.9}
{'loss': 0.3117, 'learning_rate': 5e-05, 'epoch': 8.98}
{'eval_loss': 0.7645336985588074, 'eval_acc': 0.7370030581039755, 'eval_runtime': 35.3479, 'eval_samples_per_second': 92.509, 'eval_steps_per_second': 5.799, 'epoch': 9.0}
{'loss': 0.3248, 'learning_rate': 5e-05, 'epoch': 9.07}
{'loss': 0.2798, 'learning_rate': 5e-05, 'epoch': 9.15}
{'loss': 0.2885, 'learning_rate': 5e-05, 'epoch': 9.24}
{'loss': 0.3069, 'learning_rate': 5e-05, 'epoch': 9.32}
{'loss': 0.3281, 'learning_rate': 5e-05, 'epoch': 9.41}
{'loss': 0.3207, 'learning_rate': 5e-05, 'epoch': 9.49}
{'loss': 0.2951, 'learning_rate': 5e-05, 'epoch': 9.58}
{'loss': 0.3443, 'learning_rate': 5e-05, 'epoch': 9.66}
{'loss': 0.3048, 'learning_rate': 5e-05, 'epoch': 9.75}
{'loss': 0.3535, 'learning_rate': 5e-05, 'epoch': 9.83}
{'loss': 0.3142, 'learning_rate': 5e-05, 'epoch': 9.92}
{'loss': 0.3556, 'learning_rate': 5e-05, 'epoch': 10.0}
{'eval_loss': 0.7688733339309692, 'eval_acc': 0.727217125382263, 'eval_runtime': 35.6231, 'eval_samples_per_second': 91.794, 'eval_steps_per_second': 5.755, 'epoch': 10.0}
{'train_runtime': 2702.9622, 'train_samples_per_second': 34.877, 'train_steps_per_second': 2.183, 'train_loss': 0.3525637136879614, 'epoch': 10.0}
{'eval_loss': 0.7688733339309692, 'eval_acc': 0.727217125382263, 'eval_runtime': 35.5138, 'eval_samples_per_second': 92.077, 'eval_steps_per_second': 5.772, 'epoch': 10.0}

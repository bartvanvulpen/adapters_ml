{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = 'results/'\n",
    "\n",
    "with open(RESULTS_DIR + 'exp_setup.json') as json_file:\n",
    "    exp_setup = json.load(json_file)\n",
    "    \n",
    "df = pd.read_csv(RESULTS_DIR + 'metatest_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_few_shot(acc_dict, name, color=None, ax=None):\n",
    "    sns.set()\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1,1,figsize=(5,3))\n",
    "    ks = sorted(list(acc_dict.keys()))\n",
    "    mean_accs = [acc_dict[k][0] for k in ks]\n",
    "    std_accs = [acc_dict[k][1] for k in ks]\n",
    "    ax.plot(ks, mean_accs, marker='o', markeredgecolor='k', markersize=6, label=name, color=color)\n",
    "    ax.fill_between(ks, [m-s for m,s in zip(mean_accs, std_accs)], [m+s for m,s in zip(mean_accs, std_accs)], alpha=0.2, color=color)\n",
    "    ax.set_xticks(ks)\n",
    "    ax.set_xlim([ks[0]-1, ks[-1]+1])\n",
    "    ax.set_xlabel(\"Number of shots per class\", weight='bold')\n",
    "    ax.set_ylabel(\"Accuracy\", weight='bold')\n",
    "    if len(ax.get_title()) == 0:\n",
    "        ax.set_title(\"Few-Shot Performance \" + name, weight='bold')\n",
    "    else:\n",
    "        ax.set_title(ax.get_title() + \" and \" + name, weight='bold')\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_nrs = df['exp_nr'].unique()\n",
    "labels = df['metatest'].unique()\n",
    "marker=['o', 's', '*', '+', 'd', 'v', '<', '^', '>']\n",
    "marker_map = dict(zip(labels, marker))\n",
    "\n",
    "for nr in exp_nrs:\n",
    "    if nr == '1':\n",
    "        continue\n",
    "    if len(nr) > 1:\n",
    "        nr_setup = nr[0]\n",
    "    else:\n",
    "        nr_setup = nr\n",
    "    \n",
    "        \n",
    "    setup = exp_setup[nr_setup]\n",
    "    \n",
    "    exp_results = df.loc[df['exp_nr'] ==  nr]\n",
    "    tasks = exp_results['metatest'].unique()\n",
    "    \n",
    "    for task in tasks:\n",
    "        task_df = exp_results.loc[exp_results['metatest'] ==  task]\n",
    "        \n",
    "        \n",
    "        accs = task_df['avg'].values\n",
    "        stds = task_df['std'].values\n",
    "        ks = task_df['K'].values\n",
    "        \n",
    "        plt.plot(ks, accs,  marker=marker_map[task], markeredgecolor='k', markersize=8)\n",
    "        plt.fill_between(ks, [m-s for m,s in zip(accs, stds)], [m+s for m,s in zip(accs, stds)], alpha=0.2, color='red')\n",
    "        \n",
    "    plt.xticks(ks)   \n",
    "    plt.xlabel('k')\n",
    "    plt.legend(tasks)\n",
    "    plt.ylabel('Mean accuracy')\n",
    "    plt.title(nr)\n",
    "    print(\"Adapters used:\", setup['adapters'])\n",
    "    print(\"Meta-trained on:\", setup['metatrain'])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
